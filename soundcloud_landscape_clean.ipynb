{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infinitely scrolls to the bottom of the page given by url. Assumes a chromedriver window is already open.\n",
    "#stop_scroll does nothing lol\n",
    "\n",
    "def get_html_scroll(url, stop_scroll):\n",
    "    driver.get(url)\n",
    "    \n",
    "    #determines the intervals between jumps to the bottom of the page. could be decreased if you're internet is \n",
    "    #particularly fast\n",
    "    SCROLL_PAUSE_TIME = .5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for i in range(50):\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # If it has reached the bottom of the page, wait three more seconds in case it's just loading.\n",
    "        # If no more page has loaded after 3 seconds, return the pages html.\n",
    "        \n",
    "        if new_height == last_height: \n",
    "            time.sleep(3)  \n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "        last_height = new_height\n",
    "        \n",
    "    return driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a list of artist names (more precisely soundcloud's internal marker for them) contained in the HTML \n",
    "#of a likes page e.g. https://soundcloud.com/pswjt/likes The fact it concatenates this list to itself \n",
    "#serves no real purpose before returning serves no real purpose.\n",
    "\n",
    "def parse_likes(html):\n",
    "    links = re.findall('<a class=\"sound__coverArt\" href=\"/\\S{,50}/', html)\n",
    "    links = [str[34:len(str)-1].split(\"/\")[0] for str in links]\n",
    "    return np.column_stack((links, links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same thing but for reposts\n",
    "\n",
    "def parse_reposts(html):\n",
    "    links = re.findall('<a class=\"sound__coverArt\" href=\"/.{,100}/', html)\n",
    "    links = [str[34:len(str)-1].split(\"/\")[0] for str in links]\n",
    "    return np.column_stack((links,links))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same thing but for comments\n",
    "\n",
    "def parse_comments(html):\n",
    "    links = re.findall('a class=\"sc-link-light\" href=\"/.{,100}/', html)\n",
    "    links = [str[31:len(str)-1].split(\"/\")[0] for str in links]\n",
    "    return np.column_stack((links,links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same thing but for follows\n",
    "\n",
    "def parse_follows(html):\n",
    "    links = re.findall('<a href=\"/.{,100}\" class=\"userBadgeListItem__image\">', html)\n",
    "    links = [str[10:len(str)-35] for str in links]\n",
    "    return np.column_stack((links,links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the number of followers a user has. Takes in the HTML of an artists base profile.\n",
    "\n",
    "def parse_followers(html):\n",
    "    followers = re.findall('\"followers_count\":\\d{,50},', html)\n",
    "    num = int(followers[0][18:len(followers[0])-1])\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a user's display name, as opposed to soundcloud's internal marker for them. HTML is for the base profile.\n",
    "\n",
    "def parse_name(html):\n",
    "    name = re.findall('\"username\":\".{,200}\"', html_followers)[0]\n",
    "    name = name[12:]\n",
    "    name = name.split('\\\"')[0]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks if the artists in arr are already in the seeds dataframe, and adds them if not.\n",
    "#arr is an array of artist url markers returned by the parse functions.\n",
    "\n",
    "def add_seeds(arr, seeds): \n",
    "    \n",
    "    for i in range(arr.shape[0]): #iterate through every artist passed to function\n",
    "        \n",
    "        search = seeds['links'] == arr[i,1] \n",
    "        if not any(search): #if artist not already in list, add to list\n",
    "            \n",
    "            #fills 'names' field with soundcloud's internal marker for now. will be changed when\n",
    "            #an account is actually scraped.\n",
    "            new_line = pd.DataFrame(data={'names': arr[i,0], 'links': arr[i,1], 'followers': 0}, index=[0])\n",
    "            seeds = seeds.append(new_line, ignore_index=True)\n",
    "\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a given dataframe and artist name, and appends that artist's data to the dataframe. df can be any of likes_df, \n",
    "#comments_df, reposts_df, or follows_df. i is internal soundcloud signifier for the artist (bad variable name ik),\n",
    "#to_collect is a string representing the type of data to be collected. \n",
    "\n",
    "def scrape_data(df, to_collect, seeds, i, followers):\n",
    "    \n",
    "    #does not scrape data if followers is less than 250\n",
    "    if followers >= 250: \n",
    "        \n",
    "        #contstructs appropriate url given the artist marker and type of data to collect.\n",
    "        url = \"https://soundcloud.com/\" + i + \"/\" + to_collect \n",
    "        \n",
    "        #opens page and scrolls at least 50 times to bottom, gets html source code\n",
    "        html = get_html_scroll(url,False)\n",
    "\n",
    "        #clunky but it works. data = array of artists found in each respective page\n",
    "        if to_collect == 'likes':\n",
    "            data = parse_likes(html)\n",
    "        elif to_collect == 'comments':\n",
    "            data = parse_comments(html)\n",
    "        elif to_collect == 'following':\n",
    "            data = parse_follows(html)\n",
    "        else:\n",
    "            data = parse_reposts(html)\n",
    "\n",
    "        #adds unique artists found to seeds list\n",
    "        seeds = add_seeds(data, seeds)\n",
    "        \n",
    "    #these lines add a row with all zeros to the dataframe for each new artist scraped, as well as adding columns to the\n",
    "    #dataframe. because it reconstructs the column names from the seeds dataframe every time a new artist is scraped, \n",
    "    #be careful to make sure order of columns labels and seeds stay the same. deleting a column from a dataframe\n",
    "    #but not the seeds list will scramble the data \n",
    "    \n",
    "    pad_len = seeds.shape[0]-df.shape[1]\n",
    "    \n",
    "    pad_arr = np.pad(df.values,((0,1), (0,pad_len)),mode='constant')\n",
    "        \n",
    "    df = pd.DataFrame(pad_arr, index=df.index.append(pd.Index([i])), columns=seeds['links'])\n",
    "\n",
    "    #if an account's followers are less than 250, simply leave the row of all zeros unchanged.\n",
    "    #these will be filtered out later.\n",
    "    \n",
    "    if followers >= 250:\n",
    "        list = data[:,1]\n",
    "    \n",
    "        #iterates through every interactions and iterates the relevant data point\n",
    "        for j in list:\n",
    "            df.loc[str(i), str(j)] += 1\n",
    "        \n",
    "    return (df, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers</th>\n",
       "      <th>links</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8656</td>\n",
       "      <td>kggn</td>\n",
       "      <td>kuru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11328</td>\n",
       "      <td>axxturel</td>\n",
       "      <td>axxturel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>701</td>\n",
       "      <td>cargoboym</td>\n",
       "      <td>cargoboym</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   followers      links      names\n",
       "0       8656       kggn       kuru\n",
       "1      11328   axxturel   axxturel\n",
       "2        701  cargoboym  cargoboym"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DO NOT RUN THIS BLOCK UNLESS WOU WANT TO LOSE ALL YOUR CURRENT DATA\n",
    "\n",
    "#This block basically resets progress, and initializes the dataframes to \n",
    "#all zeros with the columns and rows labeled by seeds.csv\n",
    "\n",
    "seeds = pd.read_csv(\"seeds.csv\")\n",
    "num = seeds.shape[0]\n",
    "\n",
    "likes_df = pd.DataFrame(np.zeros((num,num)), index=seeds['links'], columns=seeds['links'])\n",
    "comments_df = pd.DataFrame(np.zeros((num,num)), index=seeds['links'], columns=seeds['links'])\n",
    "follows_df = pd.DataFrame(np.zeros((num,num)), index=seeds['links'], columns=seeds['links'])\n",
    "reposts_df = pd.DataFrame(np.zeros((num,num)), index=seeds['links'], columns=seeds['links'])\n",
    "\n",
    "file = open(\"tracking_num.txt\",\"w\")\n",
    "file.write(str(0))\n",
    "file.close()\n",
    "\n",
    "print(likes_df.shape)\n",
    "seeds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this block to reload data after closing and saving progress. Don't run this block starting the program for\n",
    "#the first time\n",
    "\n",
    "likes_df = pd.read_parquet(\"likes_df.parquet\")\n",
    "\n",
    "comments_df = pd.read_parquet(\"comments_df.parquet\")\n",
    "\n",
    "follows_df = pd.read_parquet(\"follows_df.parquet\")\n",
    "\n",
    "reposts_df = pd.read_parquet(\"reposts_df.parquet\")\n",
    "\n",
    "#remaining_seeds is the sorted list I used to determine scraping order around halfway \n",
    "#through. because columns are continuously relabeled with the seeds dataframe, it still\n",
    "#needed to be kept.\n",
    "\n",
    "remaining_seeds = pd.read_csv(\"remaining_seeds.csv\")\n",
    "\n",
    "seeds = pd.read_csv(\"seeds.csv\")\n",
    "\n",
    "#num is the row number in remaining_seeds that the program is currently at. written to a txt \n",
    "#file to track progress between scraping sessions\n",
    "\n",
    "file = open(\"tracking_num.txt\",\"r\")\n",
    "num = int(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490\n",
      "117\n",
      "470\n",
      "480\n",
      "470\n",
      "450\n",
      "500\n",
      "258\n",
      "318\n",
      "20\n",
      "480\n",
      "470\n",
      "460\n",
      "29\n",
      "460\n",
      "460\n",
      "470\n",
      "14\n",
      "470\n",
      "470\n",
      "469\n",
      "50\n",
      "62\n",
      "257\n",
      "450\n",
      "490\n",
      "52\n",
      "0\n",
      "57\n",
      "440\n",
      "480\n",
      "94\n",
      "1\n",
      "19\n",
      "480\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.04 GiB for an array with shape (2243, 121894) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-70d46c51020b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mcomments_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomments_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'comments'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollowers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mfollows_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfollows_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'following'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollowers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mreposts_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreposts_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reposts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollowers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mnum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-70a34d2c19b4>\u001b[0m in \u001b[0;36mscrape_data\u001b[1;34m(df, to_collect, seeds, i, followers)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mpad_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mpad_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpad_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'constant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'links'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpad\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[1;31m# Create array with final shape and original values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[1;31m# (padded area is undefined)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m     \u001b[0mpadded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_area_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pad_simple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m     \u001b[1;31m# And prepare iteration over all dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m     \u001b[1;31m# (zipping may be more readable than using enumerate)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraypad.py\u001b[0m in \u001b[0;36m_pad_simple\u001b[1;34m(array, pad_width, fill_value)\u001b[0m\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m     \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'F'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfnc\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'C'\u001b[0m  \u001b[1;31m# Fortran and not also C-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[0mpadded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.04 GiB for an array with shape (2243, 121894) and data type int64"
     ]
    }
   ],
   "source": [
    "#The main block containing the for loop controlling the program\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe') \n",
    "\n",
    "while num < 10000: #arbitrary choice of number here, did not even get close to 10000\n",
    "    \n",
    "    #if you would like to retain original depth first type behavior (meaning scrape artists the order\n",
    "    #in which they're found), this line can be changed to \n",
    "    #i = seeds.iloc[num]['links']\n",
    "    \n",
    "    i = remaining_seeds.iloc[num]['links']\n",
    "    \n",
    "    #get artist name and followers first\n",
    "    #I should add some form of error handling here since a few times it crashed if the sc link led nowhere\n",
    "    \n",
    "    url = \"https://soundcloud.com/\" + i\n",
    "    driver.get(url)\n",
    "    html_followers = driver.page_source\n",
    "    followers = parse_followers(html_followers)\n",
    "    remaining_seeds.at[num, 'followers'] = followers #also change this to seeds to get depth-first behavior\n",
    "    \n",
    "    name = parse_name(html_followers)\n",
    "    remaining_seeds.at[num, 'names'] = name #ditto ^\n",
    "    \n",
    "    likes_df, seeds = scrape_data(likes_df, 'likes', seeds, i, followers)\n",
    "    comments_df, seeds = scrape_data(comments_df, 'comments', seeds, i, followers)\n",
    "    follows_df, seeds = scrape_data(follows_df, 'following', seeds, i, followers)\n",
    "    reposts_df, seeds = scrape_data(reposts_df, 'reposts', seeds, i, followers)\n",
    "\n",
    "    num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell after terminating program to save progress. Try to only stop the program when it's\n",
    "#still on an artist's likes page to prevent a dimensional mismatch\n",
    "\n",
    "likes_df.to_parquet(\"likes_df.parquet\")\n",
    "comments_df.to_parquet(\"comments_df.parquet\")\n",
    "follows_df.to_parquet(\"follows_df.parquet\")\n",
    "reposts_df.to_parquet(\"reposts_df.parquet\")\n",
    "\n",
    "remaining_seeds.to_csv(\"remaining_seeds.csv\", index=False)\n",
    "\n",
    "seeds.to_csv(\"seeds.csv\", index=False)\n",
    "\n",
    "file = open(\"tracking_num.txt\",\"w\")\n",
    "file.write(str(num))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual plotting/data cleanup happens in the soundcloud_plotting_clean.ipynb file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
